{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import operator\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000\n",
      "Loaded 200000\n",
      "Loaded 300000\n",
      "Loaded 400000\n",
      "Loaded 500000\n",
      "Loaded 600000\n",
      "Loaded 700000\n",
      "Loaded 800000\n",
      "Loaded 900000\n",
      "total loaded sessions are: 980503\n"
     ]
    }
   ],
   "source": [
    "with open(\"train-queries.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=';')\n",
    "    sess_clicks = {}\n",
    "    sess_date = {}\n",
    "    sess_tokens = {}\n",
    "    sess_divider = {}\n",
    "    ctr = 0\n",
    "    trainset = []\n",
    "    testset = []\n",
    "    for data in reader:\n",
    "        sessid = data['queryId'] #query id\n",
    "        #sessid = data['sessionId'] #session id\n",
    "        items = data['items'] #string of items id\n",
    "        clicks = items.split(',')\n",
    "        sess_clicks[sessid] = clicks\n",
    "        tokens = data['searchstring.tokens'] #string of search tokens\n",
    "        searches = tokens.split(',')\n",
    "        sess_tokens[sessid] = searches\n",
    "        curdate = data['eventdate'] #time \n",
    "        date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n",
    "        sess_date[sessid] = date\n",
    "        istest = data['is.test']\n",
    "        sess_divider[sessid] = istest\n",
    "        ctr += 1\n",
    "#         if istest == 'FALSE':\n",
    "#             trainset += [sessid]\n",
    "#         else:\n",
    "#             testset += [sessid]\n",
    "        if ctr % 100000 == 0:\n",
    "            print ('Loaded', ctr)\n",
    "            #break\n",
    "    print('total loaded sessions are:', sessid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out length 1 sessions\n",
    "store_clicks = {}\n",
    "store_date = {}\n",
    "store_tokens = {}\n",
    "store_divider = {}\n",
    "for s in sess_clicks.keys():\n",
    "    if len(sess_clicks[s]) != 1:\n",
    "        store_clicks[s]=sess_clicks[s]\n",
    "        store_date[s]=sess_date[s]\n",
    "        store_tokens[s] = sess_tokens[s]\n",
    "        store_divider[s] = sess_divider[s]\n",
    "sess_clicks = store_clicks\n",
    "sess_date = store_date\n",
    "sess_tokens = store_tokens\n",
    "sess_divider = store_divider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of times each item appears\n",
    "iid_counts = {}\n",
    "for s in sess_clicks:\n",
    "    seq = sess_clicks[s]\n",
    "    for iid in seq:\n",
    "        if iid in iid_counts:\n",
    "            iid_counts[iid] += 1\n",
    "        else:\n",
    "            iid_counts[iid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows how many times each item has appeared in all sessions in a sorted manner\n",
    "sorted_counts = sorted(iid_counts.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# Filter out those items appear less than 5 times in all sessions\n",
    "store_clicks = {}\n",
    "store_date = {}\n",
    "store_tokens = {}\n",
    "store_divider = {}\n",
    "for s in sess_clicks.keys():\n",
    "    curseq = sess_clicks[s]\n",
    "    #Filter out those items that shows less than 5 times in total\n",
    "    filseq = list(filter(lambda i: iid_counts[i] >= 5, curseq))   \n",
    "    # Keep those session that longer than 2 items after filtering and dump others\n",
    "    if len(filseq) >= 2:\n",
    "        store_clicks[s] = filseq\n",
    "        store_date[s] = sess_date[s]\n",
    "        store_tokens[s] = sess_tokens[s]\n",
    "        store_divider[s] = sess_divider[s]\n",
    "# Update sess_clisks and sess_date\n",
    "sess_clicks = store_clicks\n",
    "sess_date = store_date\n",
    "sess_tokens = store_tokens\n",
    "sess_divider = store_divider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set and test set per sess_divider{}\n",
    "for key in sess_divider:\n",
    "    istest = sess_divider[key]\n",
    "    if istest == 'FALSE':\n",
    "        trainset += [key]\n",
    "    else:\n",
    "        testset += [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_items = {}\n",
    "train_tokens = {}\n",
    "train_date = {}\n",
    "for key in trainset:\n",
    "    train_items[key] = sess_clicks[key]\n",
    "    train_tokens[key] = sess_tokens[key]\n",
    "    train_date[key] = sess_date[key]\n",
    "test_items = {}\n",
    "test_tokens = {}\n",
    "test_date = {}\n",
    "for key in testset:\n",
    "    test_items[key] = sess_clicks[key]\n",
    "    test_tokens[key] = sess_tokens[key]\n",
    "    test_date[key] = sess_date[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total number of sessions in the training set is: 633454\n"
     ]
    }
   ],
   "source": [
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "item_dict = {}\n",
    "token_dict = {}\n",
    "item_ctr = 1\n",
    "token_ctr = 1\n",
    "train_seqs = []\n",
    "train_dates = []\n",
    "train_tks = []\n",
    "for s, date in train_date.items():\n",
    "    seq = train_items[s]\n",
    "    tk_seq = train_tokens[s]\n",
    "    outseq = []\n",
    "    tk_outseq = []\n",
    "    for i in seq:\n",
    "        if i in item_dict:\n",
    "            outseq += [item_dict[i]]\n",
    "        else:\n",
    "            outseq += [item_ctr]\n",
    "            item_dict[i] = item_ctr\n",
    "            item_ctr += 1\n",
    "            \n",
    "    for i in tk_seq:   \n",
    "        if i in token_dict:\n",
    "            tk_outseq += [token_dict[i]]\n",
    "        else:\n",
    "            tk_outseq += [token_ctr]\n",
    "            token_dict[i] = token_ctr\n",
    "            token_ctr += 1\n",
    "            \n",
    "    train_seqs += [outseq]\n",
    "    train_dates += [date]\n",
    "    train_tks += [tk_outseq]\n",
    "print(\"\\ntotal number of sessions in the training set is:\", len(train_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sessions in the test set is: 285892\n",
      "\n",
      "total number of different items in the training set is: 123422\n"
     ]
    }
   ],
   "source": [
    "store_tokens = {}\n",
    "test_seqs = []\n",
    "test_dates = []\n",
    "test_tks = []\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "for s, date in test_date.items():\n",
    "    seq = test_items[s]\n",
    "    tk_seq = test_tokens[s]\n",
    "#     test_tks += [test_tokens[s]]\n",
    "    outseq = []\n",
    "    tk_outseq = []\n",
    "    for i in seq:\n",
    "        if i in item_dict:\n",
    "            outseq += [item_dict[i]]\n",
    "    if len(outseq) < 2:\n",
    "        continue\n",
    "    else:\n",
    "        #Filter out corresponding test_tokens\n",
    "        store_tokens[s] = test_tokens[s]    \n",
    "    test_seqs += [outseq]\n",
    "    test_dates += [date]        \n",
    "for s in store_tokens:  \n",
    "    for i in tk_seq:\n",
    "        if i in token_dict:\n",
    "            tk_outseq += [token_dict[i]]\n",
    "    test_tks += [tk_outseq]\n",
    "print(\"total number of sessions in the test set is:\", len(test_date))\n",
    "print(\"\\ntotal number of different items in the training set is:\", len(item_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285892\n",
      "285892\n",
      "285892\n"
     ]
    }
   ],
   "source": [
    "print(len(test_seqs))\n",
    "print(len(test_dates))\n",
    "print(len(test_tks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation and labels setting\n",
    "def process_seqs(iseqs, idates, itokens):\n",
    "    out_seqs = []\n",
    "    out_dates = []\n",
    "    out_tokens = []\n",
    "    labs = []\n",
    "    for seq, date in zip(iseqs, idates):\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "            out_dates += [date]        \n",
    "    for i in range(len(itokens)):\n",
    "        for t in range(len(iseqs[i]) - 1):\n",
    "            out_tokens += [itokens[i]]\n",
    "    return out_seqs, out_dates, out_tokens, labs\n",
    "\n",
    "tr_seqs, tr_dates, tr_tokens, tr_labs = process_seqs(train_seqs,train_dates, train_tks)\n",
    "te_seqs, te_dates, te_tokens, te_labs = process_seqs(test_seqs,test_dates, test_tks)\n",
    "train = (tr_seqs, tr_labs)\n",
    "test = (te_seqs, te_labs)\n",
    "print('\\nAfter augmentation, total number of sessions in the training set is: ', len(tr_seqs))\n",
    "print('\\nAfter augmentation, total number of sessions in the test set is: ', len(te_seqs))\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Used to shuffle the dataset at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if minibatch_start != n:\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_minibatches(seqs, name):\n",
    "    lengths = [len(i) for i in seqs]\n",
    "    maxlen=np.max(lengths)\n",
    "    n_samples = len(seqs)\n",
    "    \n",
    "    x = np.zeros((maxlen, n_samples))\n",
    "    X_mask = np.ones((maxlen, n_samples))\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x[-lengths[idx]:, idx] = s\n",
    "\n",
    "    X_mask *= (1 - (x == 0))\n",
    "\n",
    "    X = np.transpose(x)\n",
    "    X_mask = np.transpose(X_mask)\n",
    "\n",
    "    aa=get_minibatches_idx(len(X), 512, shuffle=False)\n",
    "    b=0\n",
    "    bb=[]\n",
    "    X = X.astype(int)\n",
    "\n",
    "    for _, train_index in aa:\n",
    "        bb.append(train_index)\n",
    "        b+=1\n",
    "    bb.remove(bb[-1])\n",
    "    print('\\nNumber of minibatches of ' + name + ' is: ', len(bb))\n",
    "    return X, X_mask, bb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs=np.array(tr_labs)\n",
    "labs_test=np.array(te_labs)\n",
    "X, X_mask, bb = divide_minibatches(tr_seqs, 'training set')\n",
    "X_test, X_test_mask, bb_test = divide_minibatches(te_seqs, 'test set')\n",
    "X_tr_tokens, X_tr_tokens_mask, bb_tr_tokens = divide_minibatches(tr_tokens, 'tr_tokens set')\n",
    "X_te_tokens, X_te_tokens_mask, bb_te_tokens = divide_minibatches(te_tokens, 'te_tokens set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2}\n"
     ]
    }
   ],
   "source": [
    "A = {'a':1,'b':2}\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n"
     ]
    }
   ],
   "source": [
    "B = list(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
